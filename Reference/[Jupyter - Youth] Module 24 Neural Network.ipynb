{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Chatbot with Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've discovered how to build a chatbot with cosine similarity. Now, let's explore how we might build one with neural network!\n",
    "\n",
    "We will create our training data, train a neural network with them, then use the trained model to make our chatbot. \n",
    "\n",
    "First, we will install required libraries. Uncomment the few blocks below only if you do not have the libraries installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.20.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.0-cp38-cp38-win_amd64.whl (33.7 MB)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.7.0\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp38-cp38-win_amd64.whl (6.9 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.7.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n",
      "Collecting pillow\n",
      "  Downloading Pillow-8.2.0-cp38-cp38-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: pillow\n",
      "Successfully installed pillow-8.2.0\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.3.0-cp38-cp38-win_amd64.whl (2.8 MB)\n",
      "Requirement already satisfied: numpy>=1.17.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from h5py) (1.20.1)\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy\n",
    "!pip install scikit-learn\n",
    "!pip install pillow\n",
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Libraries\n",
    "\n",
    "Firstly, we will install libraries needed for this neural network powered chatbot. \n",
    "Keras is a machine learning library which utilizes tensorflow (another lower level machine learning library) at the backend. This makes it easier for us to deploy deep neural network for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dense\n",
    " \n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Input training data\n",
    "\n",
    "We will first include the following training data for our chatbot:\n",
    "1. X represent the different possible inputs that users might enter\n",
    "2. Y represent the intent of the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['Hi',\n",
    "     'Hello',\n",
    "     'How are you?',\n",
    "     'I am making',\n",
    "     'making',\n",
    "     'working',\n",
    "     'studying',\n",
    "     'see you later',\n",
    "     'bye',\n",
    "     'goodbye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ['greeting',\n",
    "     'greeting',\n",
    "     'greeting',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'bye',\n",
    "     'bye',\n",
    "     'bye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are several different sentences that have similar intent. Here, we are only having 3 intents, but you can add as many as you want for your project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the way our chatbot will work:\n",
    "1. From the input sentence, we will identify the intent using our trained AI model.\n",
    "2. For each intent, we have a prepared response. \n",
    "\n",
    "For example, if we identify that the intent of the input is for a greeting, we might ask the chatbot to reply with a greeting as well, something like 'hi' or 'how are you doing?'\n",
    "\n",
    "We will use machine learning to create a model that can classify input sentence into different intents. \n",
    "We make it as follows:\n",
    "\n",
    "1. We create a training data (X and Y above) which contains a list of sentences and their intents.\n",
    "2. Use the training data to train a classifier. \n",
    "3. Vectorize input sentences and use classifier to determine intent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text processing\n",
    "\n",
    "As usual, we will start with text processing. Do you remember the process?\n",
    "\n",
    "## 3.1 Remove non alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alpha_numeric_characters(sentence):\n",
    "    new_sentence = ''\n",
    "    for alphabet in sentence:\n",
    "        if alphabet.isalpha() or alphabet == ' ':\n",
    "            new_sentence += alphabet\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    X = [data_point.lower() for data_point in X]\n",
    "    X = [remove_non_alpha_numeric_characters(\n",
    "        sentence) for sentence in X]\n",
    "    X = [data_point.strip() for data_point in X]\n",
    "    X = [re.sub(' +', ' ',data_point) for data_point in X]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_data(X)\n",
    "\n",
    "vocabulary = set()\n",
    "for data_point in X:\n",
    "    for word in data_point.split(' '):\n",
    "        vocabulary.add(word)\n",
    "\n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = []\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    sentence = preprocess_data([sentence])[0]\n",
    "    sentence_encoded = [0] * len(vocabulary)\n",
    "    for i in range(len(vocabulary)):\n",
    "        if vocabulary[i] in sentence.split(' '):\n",
    "            sentence_encoded[i] = 1\n",
    "    return sentence_encoded\n",
    "\n",
    "X_encoded = [encode_sentence(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(Y))\n",
    "\n",
    "Y_encoded = []\n",
    "for data_point in Y:\n",
    "    data_point_encoded = [0] * len(classes)\n",
    "    for i in range(len(classes)):\n",
    "        if classes[i] == data_point:\n",
    "            data_point_encoded[i] = 1\n",
    "    Y_encoded.append(data_point_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_encoded\n",
    "y_train = Y_encoded\n",
    "X_test = X_encoded\n",
    "y_test = Y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and check the data you are using for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does y_train represent? Do you understand the array shown above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the training data to train our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='sigmoid',\n",
    "                input_dim=len(X_train[0])))\n",
    "model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.01,\n",
    "                            momentum=0.9, nesterov=True))\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=200, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List down predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [argmax(pred) for pred in model.predict(np.array(X_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model now. We will compare the prediction made by the model and our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == argmax(y_test[i]):\n",
    "        correct += 1\n",
    "\n",
    "print (\"Correct:\", correct)\n",
    "print (\"Total:\", len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the chatbot now! We will input a sentence, and then see what class is predicted by the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    print (\"Enter a sentence\")\n",
    "    sentence = input()\n",
    "    if sentence == 'exit':\n",
    "        break\n",
    "    prediction= model.predict(np.array([encode_sentence(sentence)]))\n",
    "    print (classes[argmax(prediction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realize that you can't stop the chatbot? You'll have to add the exit command later (see the previous notebook to find out how to do it. \n",
    "\n",
    "For now, simply press the stop button (interrupt button) above to stop the chatbot. \n",
    "\n",
    "Try it! press the stop button, and try typing something onto the box. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "We have successfully use neural network to map our input to conversation intent. \n",
    "Your challenge is to link the conversation intent to a particular response that the chatbot will say. \n",
    "For example, if the conversation intent is 'greeting', get your chatbot to say a greeting as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great job! You've successfully created a simple chatbot with neural network! How might you improve the chatbot?\n",
    "You can improve the chatbot by:\n",
    "- Adding more training data\n",
    "- Adding more intent\n",
    "- Focusing on a particular topic and train the chatbot with many training data in that topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource:\n",
    "https://blog.eduonix.com/internet-of-things/simple-nlp-based-chatbot-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "f23faf4bfe871c203c8bec80520af5927fc7cb1ae3bd834ddf554ee587ad1c05"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
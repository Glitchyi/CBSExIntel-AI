{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of text data\n",
    "\n",
    "One popular application of Natural Language processing is to classify text data. Classification can be done in various form: we can classify if a tweet is related to a particular topic or not, or we can classify if a particular review leans towards positive or negative. \n",
    "\n",
    "Today, we will hone our skills in classifying NLP data using NLP! \n",
    "Firstly, we will collect, analyze and process our data. We will be using bag of words and tf-idf (remember the activity that we have done in the acquire stage?). These processes turn texts into numbers. \n",
    "We will then take the word vectors we have created and use a machine learning algorithm to learn from the data and come up with a model to do our classification task. \n",
    "\n",
    "Let's start!\n",
    "\n",
    "For our first task, we will use a collection of tweet data to predict if the tweets are referring to natural disasters, or just regular tweets.\n",
    "\n",
    "Let's first import the required libraries!\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open csv file\n",
    "Do you have the tweet data file with you? If not, refer to Experience module 1 to see how you can download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('socialmedia-disaster-tweets-DFE.csv', encoding='latin-1') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our data for classification\n",
    "Before we begin working with the data, let us first look at some of the data's features. You should have an idea of the data structure from the data analysis in Experience 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0   _unit_id  _golden _unit_state  _trusted_judgments  \\\n",
      "0               0  778243823     True      golden                 156   \n",
      "1               1  778243824     True      golden                 152   \n",
      "2               2  778243825     True      golden                 137   \n",
      "3               3  778243826     True      golden                 136   \n",
      "4               4  778243827     True      golden                 138   \n",
      "...           ...        ...      ...         ...                 ...   \n",
      "10871       10871  778261105     True      golden                 100   \n",
      "10872       10872  778261106     True      golden                  90   \n",
      "10873       10873  778261107     True      golden                 102   \n",
      "10874       10874  778261108     True      golden                  96   \n",
      "10875       10875  778261109     True      golden                  97   \n",
      "\n",
      "      _last_judgment_at choose_one  choose_one:confidence choose_one_gold  \\\n",
      "0                   NaN   Relevant                 1.0000        Relevant   \n",
      "1                   NaN   Relevant                 1.0000        Relevant   \n",
      "2                   NaN   Relevant                 1.0000        Relevant   \n",
      "3                   NaN   Relevant                 0.9603        Relevant   \n",
      "4                   NaN   Relevant                 1.0000        Relevant   \n",
      "...                 ...        ...                    ...             ...   \n",
      "10871               NaN   Relevant                 0.7629        Relevant   \n",
      "10872               NaN   Relevant                 0.9203        Relevant   \n",
      "10873               NaN   Relevant                 1.0000        Relevant   \n",
      "10874               NaN   Relevant                 0.8419        Relevant   \n",
      "10875               NaN   Relevant                 0.8812        Relevant   \n",
      "\n",
      "      keyword location                                               text  \\\n",
      "0         NaN      NaN                 Just happened a terrible car crash   \n",
      "1         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "2         NaN      NaN  Heard about #earthquake is different cities, s...   \n",
      "3         NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
      "4         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "...       ...      ...                                                ...   \n",
      "10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
      "10872     NaN      NaN  Police investigating after an e-bike collided ...   \n",
      "10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
      "10874     NaN      NaN  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
      "10875     NaN      NaN  #CityofCalgary has activated its Municipal Eme...   \n",
      "\n",
      "         tweetid  userid  \n",
      "0            1.0     NaN  \n",
      "1           13.0     NaN  \n",
      "2           14.0     NaN  \n",
      "3           15.0     NaN  \n",
      "4           16.0     NaN  \n",
      "...          ...     ...  \n",
      "10871  5675678.0     NaN  \n",
      "10872     4234.0     NaN  \n",
      "10873     3242.0     NaN  \n",
      "10874      457.0     NaN  \n",
      "10875     6585.0     NaN  \n",
      "\n",
      "[10876 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "print(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head() function allows us to see the first few rows from the dataset. Do you wonder how many rows do we have here?\n",
    "\n",
    "### Task: print out the length of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   _unit_id  _golden _unit_state  _trusted_judgments  \\\n",
      "0           0  778243823     True      golden                 156   \n",
      "1           1  778243824     True      golden                 152   \n",
      "2           2  778243825     True      golden                 137   \n",
      "3           3  778243826     True      golden                 136   \n",
      "4           4  778243827     True      golden                 138   \n",
      "5           5  778243828     True      golden                 140   \n",
      "6           6  778243831     True      golden                 142   \n",
      "\n",
      "  _last_judgment_at choose_one  choose_one:confidence choose_one_gold keyword  \\\n",
      "0               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "1               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "2               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "3               NaN   Relevant                 0.9603        Relevant     NaN   \n",
      "4               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "5               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "6               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "\n",
      "  location                                               text  tweetid  userid  \n",
      "0      NaN                 Just happened a terrible car crash      1.0     NaN  \n",
      "1      NaN  Our Deeds are the Reason of this #earthquake M...     13.0     NaN  \n",
      "2      NaN  Heard about #earthquake is different cities, s...     14.0     NaN  \n",
      "3      NaN  there is a forest fire at spot pond, geese are...     15.0     NaN  \n",
      "4      NaN             Forest fire near La Ronge Sask. Canada     16.0     NaN  \n",
      "5      NaN  All residents asked to 'shelter in place' are ...     17.0     NaN  \n",
      "6      NaN  13,000 people receive #wildfires evacuation or...     18.0     NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df_raw.head(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What headings do you see above? Which headings do you think are important?\n",
    "\n",
    "### What is a 'target'?\n",
    "We call the field we are trying to predict the 'target'. In this case, the target is whether the tweet is relevant to a natural disaster or irrelevant. These values are reflected in the `['choose_one']` column (See it above NOW!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember the 'labels'?\n",
    "In this dataset, the labels of the target has been filled out by human volunteers. When you are working on your own datasets in the future, you may have to label them manually, or find volunteers to do so.\n",
    "\n",
    "This is usually an expensive task to do in terms of effort and time. There are even [online platforms](https://www.mturk.com/) where you can find workers for this job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check labels\n",
    "\n",
    "Let's look at the the categories that the tweets have been classified into. To do that, we can look for the number of unique values within that column. The python's built-in function `set()` takes in a list of values and outputs the total unique values. Let us see how it works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple', 'orange', 'pears'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['apple', 'orange', 'apple', 'orange', 'pears'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of the code above? You have only 3 unique values in a list of 5, and only the unit values are printed out. \n",
    "\n",
    "### Task: Change the function such that you have 2 unique values and 6 values in the list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple', 'orenges'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['apple','apple','orenges','orenges','orenges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece of code below list down the values of the column 'choose_one', which is a measure of relevance of a particular tweet to natural disaster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Relevant' 'Relevant' 'Relevant' ... 'Relevant' 'Relevant' 'Relevant']\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.choose_one.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using the set() funciton, can you guess how many potential values might this column have? \n",
    "\n",
    "### Task: find out the number of unique relevance values on the 'choose_one' column using the set() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Can't Decide\", 'Not Relevant', 'Relevant'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "set(df_raw.choose_one.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense? A tweet can either be related to a natural disaster, not related to a natural disaster, and there are cases when the people who labelled the data cannot decide if the tweet is related to natural disaster or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we only care about predicting in a binary fashion (relevant vs not relevant), so we discard the 'Can't decide' class. Remember how we [subset data using criteria](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html) on pandas dataframe?  \n",
    "\n",
    "### Task: Subset the dataframe and only take rows that does not have 'Can't Decide' in the choose_one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset=df_raw[df_raw.choose_one.values != \"Can't Decide\"]\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Print out your dataframe and see what you have done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   _unit_id  _golden _unit_state  _trusted_judgments  \\\n",
      "0           0  778243823     True      golden                 156   \n",
      "1           1  778243824     True      golden                 152   \n",
      "2           2  778243825     True      golden                 137   \n",
      "3           3  778243826     True      golden                 136   \n",
      "4           4  778243827     True      golden                 138   \n",
      "\n",
      "  _last_judgment_at choose_one  choose_one:confidence choose_one_gold keyword  \\\n",
      "0               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "1               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "2               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "3               NaN   Relevant                 0.9603        Relevant     NaN   \n",
      "4               NaN   Relevant                 1.0000        Relevant     NaN   \n",
      "\n",
      "  location                                               text  tweetid  userid  \n",
      "0      NaN                 Just happened a terrible car crash      1.0     NaN  \n",
      "1      NaN  Our Deeds are the Reason of this #earthquake M...     13.0     NaN  \n",
      "2      NaN  Heard about #earthquake is different cities, s...     14.0     NaN  \n",
      "3      NaN  there is a forest fire at spot pond, geese are...     15.0     NaN  \n",
      "4      NaN             Forest fire near La Ronge Sask. Canada     16.0     NaN  \n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "print(df_subset.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the length of your dataframe now. Does it decreases or stay the same?\n",
    "\n",
    "### Print out the length of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10860, (10860, 14))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "len(df_subset),df_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to focus on only columns 'text' and 'choose_one'\n",
    "\n",
    "### Task: Subset the dataframe to only take the columns 'text' and 'choose_one'\n",
    "See Selecting Data Using Labels (Column Headings) from this [article](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text choose_one\n",
      "0                     Just happened a terrible car crash   Relevant\n",
      "1      Our Deeds are the Reason of this #earthquake M...   Relevant\n",
      "2      Heard about #earthquake is different cities, s...   Relevant\n",
      "3      there is a forest fire at spot pond, geese are...   Relevant\n",
      "4                 Forest fire near La Ronge Sask. Canada   Relevant\n",
      "...                                                  ...        ...\n",
      "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   Relevant\n",
      "10872  Police investigating after an e-bike collided ...   Relevant\n",
      "10873  The Latest: More Homes Razed by Northern Calif...   Relevant\n",
      "10874  MEG issues Hazardous Weather Outlook (HWO) htt...   Relevant\n",
      "10875  #CityofCalgary has activated its Municipal Eme...   Relevant\n",
      "\n",
      "[10876 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_raw[['text','choose_one']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also [map](https://chrisalbon.com/python/data_wrangling/pandas_map_values_to_values/) these values on to numbers 1 for relevant tweets, and 0 for irrelevant tweets. \n",
    "\n",
    "### Task: Map 'Relevant' into 1 and 'Irrelevant' into 0 and put it into a new column called 'relevance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = {'Relevant':1,'Not Relevant':0}\n",
    "df_raw['relevance'] = df_raw.choose_one.map(relevance) # map relevant values to the number 1, and not relevant to the value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        1.0\n",
       "2        1.0\n",
       "3        1.0\n",
       "4        1.0\n",
       "        ... \n",
       "10871    1.0\n",
       "10872    1.0\n",
       "10873    1.0\n",
       "10874    1.0\n",
       "10875    1.0\n",
       "Name: relevance, Length: 10876, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['relevance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at what we have done!\n",
    "- We've chosen only the column we are interested in, reducing the columns from 13 to just 3!\n",
    "- We've mapped 'Relevant' to 1 and 'Not Relevant' to 0\n",
    "\n",
    "Now, we will proceed with text processing, followed with bag of words and tf-idf!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The first step is to write functions to normalize and tokenize the tweets (We covered this in Experience 1). An example is given below, but you can and should improve it by utilizing the skills you have learnt previously. For example, by adding additional ignore-words, or by lemming or stemming the dataset. The more you pre-process the data, the better your model can be!  \n",
    "  \n",
    "Why do you think we choose to ignore those words in the ignore list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'how', 'are', 'you', 'today', 'it', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    global ignore_words\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is'] \n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # this replaces all special chars with ' '\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned \n",
    "\n",
    "# let us test this out!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Add at least 5 more stop words into the extract_words function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_words+=['Good','how']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Test your new stop words on the same sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'how', 'are', 'you', 'today', 'it', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "# let us test this out!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do you notice that the word 'it' is still present? do you know why?\n",
    "\n",
    "### Task: add a function to lower the case in extract_words function\n",
    "See [here](https://machinelearningmastery.com/clean-text-machine-learning-python/) on how to do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'are', 'he', 'she', 'my', 'you', 'it','how']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # this replaces all special chars with ' '\n",
    "    words = [w.lower() for w in words if w not in ignore_words]\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try again with the same sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'today', 'good', 'day']\n"
     ]
    }
   ],
   "source": [
    "# let us test this out!\n",
    "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
    "print(extract_words(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Feel free to add your own processing into the pipeline e.g. stemming or lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag of words\n",
    "Now that we have a function to pre-process our textual data, we can proceed with converting our textual data into numbers. The simplest way to do this is using the bag of words algorithm. \n",
    "\n",
    "In a bag of words, we count the number of times each word appears for each tweet and use those counts as our input data. This is accomplished in the following steps:\n",
    "1. Create a vocabulary of all words that appear in your corpus (a corpus is a collection of all your text data, i.e. all tweets)\n",
    "2. Turn that vocabulary into a vector. i.e. if there are 500 unique words in your corpus, the vector will have a length of 500, with each position corresponding to a word in the corpus.\n",
    "3. For each document (tweet), count the number of times every word appears and add those numbers into the vector. This will result in each document having its own vector of length 500, that represents all the words appearing in the document.\n",
    "\n",
    "### Example \n",
    "consider a corpus consisting of two documents: \n",
    "1. 'I love NLP', \n",
    "2. 'I love machine learning'. \n",
    "\n",
    "#### Vocbulary\n",
    "The vocabulary will be a vector of length 5, consisting of the words: \n",
    "\n",
    "'I', 'love', 'NLP', 'machine', 'learning'.  \n",
    "\n",
    "#### Vector\n",
    "The vector for the first sentence (number 1) will be: \n",
    "[1, 1, 1, 0, 0] since it contains 'I', 'love', 'NLP', but not 'machine', and 'learning'.  \n",
    "\n",
    "Can you construct the vector for 2?  \n",
    "\n",
    "Combining vectors for 1 and 2, the bag of words will be an array with vector 1 in the first row, and vector 2 in the second row. \n",
    "\n",
    "Now let us implement this algorithm!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the bag of words\n",
    "\n",
    "First, we would like to know how often each individual word appears in our dataset. \n",
    "\n",
    "We can represent this in the form of a dictionary, which has the format {'word':frequency}, where each key is a word, and the frequencies are the number of times the words appears in our dataset. \n",
    "\n",
    "If you would like to learn more about dictionaries, visit [python dictionaries](https://www.w3schools.com/python/python_dictionaries.asp).  \n",
    "\n",
    "### Hash map\n",
    "\n",
    "This dictionary is known as a hash map, and it can be built progressively by looping through each token in the document. \n",
    "\n",
    "If the token can not be found in the hash map, add the token to the hash map, and set it's frequency as 1. If the token already exists, increment the frequency by 1.  \n",
    "  \n",
    "We will do this in two functions. \n",
    "1. First, build a function called map_book that takes in a dictionary called the hash_map, as well as the tokens from a tweet, and updates the hash_map with each word in the tokens. \n",
    "2. Next, build a function (you can call it make_hash_map) that can loop through all the tweets, and calls the first function to update the hash_map.  \n",
    "  \n",
    "*Hint: You can loop through your tokens by using `for word in tokens:`.  \n",
    "You can check if the word exists in your hash_map by using `if word in hash_map:`  \n",
    "you can assess the counts of your hash map by using `hash_map[word]`. Increment this by 1 to increase the count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function map_book. Can you make sense of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequency of words\n",
    "def map_book(hash_map, tokens):\n",
    "    if tokens is not None:\n",
    "        for word in tokens:\n",
    "            # Word Exist?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] += 1\n",
    "            else:\n",
    "                hash_map[word] = 0\n",
    "\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function make_hash_map. Can you make sense of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_map(df):\n",
    "    hash_map = {}\n",
    "    for index, row in df.iterrows():\n",
    "        hash_map = map_book(hash_map, extract_words(row['text']))\n",
    "    return hash_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefining our dictionary\n",
    "\n",
    "While you can construct your bag of words using all words in all tweets, it can become too much for your computer very quickly. A good solution is to take just a few hundred or thousand of the most common words. We will redefine our dictionary to consist of just the 500 most popular tokens.  \n",
    "  \n",
    "How can we do this? Remember that we have just constructed a hash map which is a dictionary with each token as a key, and the number of times the token has appeared as the value. Build a function called frequent_vocab that takes in the hash_map and the maximum vocabulary, and returns a list of the most popular tokens as defined by the maximum vocabulary (set this to 500 for now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function frequent_vocab. Can you make sense of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function frequent_vocab with the following input: word_freq and max_features\n",
    "def frequent_vocab(word_freq, max_features): \n",
    "    counter = 0  #initialize counter with the value zero\n",
    "    vocab = []   # create an empty list called vocab\n",
    "    # list words in the dictionary in descending order of frequency\n",
    "    for key, value in sorted(word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True): \n",
    "       #loop function to get the top (max_features) number of words\n",
    "        if counter<max_features: \n",
    "            vocab.append(key)\n",
    "            counter+=1\n",
    "        else: break\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment! What happens if you change the above function to (reverse = False)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'co',\n",
       " 'http',\n",
       " 'in',\n",
       " 'i',\n",
       " 'ã',\n",
       " 's',\n",
       " 'for',\n",
       " 'on',\n",
       " 'that',\n",
       " 'with',\n",
       " 'by',\n",
       " 'at',\n",
       " 'this',\n",
       " 'https',\n",
       " 'from',\n",
       " 'be',\n",
       " 'was',\n",
       " 'â',\n",
       " 'm',\n",
       " '_',\n",
       " 'have',\n",
       " 'amp',\n",
       " 'like',\n",
       " 'as',\n",
       " 'up',\n",
       " 'just',\n",
       " 'we',\n",
       " 'me',\n",
       " 'but',\n",
       " 'so',\n",
       " 'not',\n",
       " 'your',\n",
       " 'out',\n",
       " 'no',\n",
       " 'all',\n",
       " 'will',\n",
       " 'after',\n",
       " 'when',\n",
       " 'fire',\n",
       " 'an',\n",
       " 'can',\n",
       " 'has',\n",
       " 'get',\n",
       " 'new',\n",
       " 'via',\n",
       " 'they',\n",
       " 'more',\n",
       " 'about',\n",
       " 'what',\n",
       " '2',\n",
       " 'now',\n",
       " 'or',\n",
       " 'news',\n",
       " 'people',\n",
       " 'one',\n",
       " 'who',\n",
       " 'there',\n",
       " 'over',\n",
       " 'been',\n",
       " 'do',\n",
       " 'into',\n",
       " 'don',\n",
       " 'emergency',\n",
       " 'video',\n",
       " 'disaster',\n",
       " 'would',\n",
       " '3',\n",
       " 'police',\n",
       " 'her',\n",
       " 're',\n",
       " 'his',\n",
       " 'than',\n",
       " 'u',\n",
       " 'were',\n",
       " 'still',\n",
       " 'some',\n",
       " 'suicide',\n",
       " 'body',\n",
       " '1',\n",
       " 'why',\n",
       " 'us',\n",
       " 'storm',\n",
       " 'burning',\n",
       " 'off',\n",
       " 'first',\n",
       " 'crash',\n",
       " 'time',\n",
       " 'them',\n",
       " 'rt',\n",
       " 'attack',\n",
       " 'had',\n",
       " 'got',\n",
       " 'back',\n",
       " 'california',\n",
       " 'day',\n",
       " 'know',\n",
       " 'fires',\n",
       " 'two',\n",
       " 'man',\n",
       " 'buildings',\n",
       " 'see',\n",
       " 'our',\n",
       " 'going',\n",
       " 'nuclear',\n",
       " 'world',\n",
       " 'today',\n",
       " 'bomb',\n",
       " 'love',\n",
       " 'year',\n",
       " 'hiroshima',\n",
       " 'full',\n",
       " 'go',\n",
       " '5',\n",
       " 'dead',\n",
       " 'life',\n",
       " 'youtube',\n",
       " 'watch',\n",
       " 'old',\n",
       " 'car',\n",
       " 'their',\n",
       " 'killed',\n",
       " 'train',\n",
       " 'think',\n",
       " '4',\n",
       " 'only',\n",
       " 'last',\n",
       " 'âªs',\n",
       " 'down',\n",
       " 'accident',\n",
       " 'war',\n",
       " 'make',\n",
       " 'good',\n",
       " 'being',\n",
       " 'gt',\n",
       " 'say',\n",
       " 'here',\n",
       " '2015',\n",
       " 'could',\n",
       " 'w',\n",
       " 'let',\n",
       " 'way',\n",
       " 'may',\n",
       " 'many',\n",
       " 'families',\n",
       " 'even',\n",
       " 'need',\n",
       " 'years',\n",
       " 'want',\n",
       " 'best',\n",
       " 'because',\n",
       " 've',\n",
       " 'then',\n",
       " 'll',\n",
       " 'did',\n",
       " 'bombing',\n",
       " 'too',\n",
       " 'right',\n",
       " 'collapse',\n",
       " 'home',\n",
       " 'him',\n",
       " 'forest',\n",
       " 'death',\n",
       " 'army',\n",
       " 'water',\n",
       " 'should',\n",
       " 'd',\n",
       " 'take',\n",
       " 'its',\n",
       " 'work',\n",
       " 'black',\n",
       " 'another',\n",
       " 'really',\n",
       " 'please',\n",
       " 'never',\n",
       " 'lol',\n",
       " 'wildfire',\n",
       " 'hot',\n",
       " 'mh370',\n",
       " 'look',\n",
       " 'help',\n",
       " 'bomber',\n",
       " 'fatal',\n",
       " 'am',\n",
       " 'any',\n",
       " 'live',\n",
       " 'where',\n",
       " 'northern',\n",
       " 'those',\n",
       " 'obama',\n",
       " '8',\n",
       " 'pm',\n",
       " 'japan',\n",
       " '11',\n",
       " 'school',\n",
       " 'much',\n",
       " 'feel',\n",
       " 'wild',\n",
       " 'flood',\n",
       " 'city',\n",
       " 'read',\n",
       " 'near',\n",
       " '9',\n",
       " 'stop',\n",
       " 'reddit',\n",
       " 'great',\n",
       " 'night',\n",
       " 'said',\n",
       " 'latest',\n",
       " 'before',\n",
       " '6',\n",
       " 'top',\n",
       " 'injured',\n",
       " 'homes',\n",
       " 'typhoon',\n",
       " 'services',\n",
       " 'floods',\n",
       " 'under',\n",
       " 'hope',\n",
       " 'fear',\n",
       " 'ever',\n",
       " 'during',\n",
       " 'come',\n",
       " 'legionnaires',\n",
       " 'atomic',\n",
       " 'im',\n",
       " 'house',\n",
       " 'flames',\n",
       " 'content',\n",
       " 'while',\n",
       " 'truck',\n",
       " 'post',\n",
       " 'getting',\n",
       " '10',\n",
       " 'wreck',\n",
       " 'state',\n",
       " 'p',\n",
       " '15',\n",
       " 'everyone',\n",
       " 'every',\n",
       " 'change',\n",
       " 'red',\n",
       " 'earthquake',\n",
       " 'damage',\n",
       " 'cross',\n",
       " '7',\n",
       " 'weather',\n",
       " 'summer',\n",
       " 'since',\n",
       " 'severe',\n",
       " 'plan',\n",
       " 'oil',\n",
       " 'coming',\n",
       " 'again',\n",
       " 'these',\n",
       " 'military',\n",
       " 'heat',\n",
       " 'found',\n",
       " 'food',\n",
       " 'thunderstorm',\n",
       " 'little',\n",
       " 'family',\n",
       " 'debris',\n",
       " 'without',\n",
       " 'most',\n",
       " 'through',\n",
       " 'next',\n",
       " 'natural',\n",
       " 'lightning',\n",
       " 'gonna',\n",
       " 'evacuation',\n",
       " 'cause',\n",
       " 'always',\n",
       " 'also',\n",
       " 'wounded',\n",
       " 'well',\n",
       " 'smoke',\n",
       " 'set',\n",
       " 'other',\n",
       " 'hit',\n",
       " 'does',\n",
       " 'devastated',\n",
       " 'bad',\n",
       " 'times',\n",
       " 'survive',\n",
       " 'national',\n",
       " 'looks',\n",
       " 'free',\n",
       " 'face',\n",
       " 'destroyed',\n",
       " 'boy',\n",
       " 'terrorist',\n",
       " 'story',\n",
       " 'rain',\n",
       " 'photo',\n",
       " 'malaysia',\n",
       " 'flooding',\n",
       " 'check',\n",
       " 'bloody',\n",
       " 'which',\n",
       " 'trapped',\n",
       " 'spill',\n",
       " 'someone',\n",
       " 'service',\n",
       " 'says',\n",
       " 'run',\n",
       " 'liked',\n",
       " 'injuries',\n",
       " 'until',\n",
       " 'show',\n",
       " 'oh',\n",
       " 'loud',\n",
       " 'landslide',\n",
       " 'fall',\n",
       " 'failure',\n",
       " '70',\n",
       " '08',\n",
       " 'âª',\n",
       " 'women',\n",
       " 'thunder',\n",
       " 'saudi',\n",
       " 'movie',\n",
       " 'game',\n",
       " 'call',\n",
       " 'boat',\n",
       " 'around',\n",
       " 'week',\n",
       " 'weapon',\n",
       " 'tonight',\n",
       " 'terrorism',\n",
       " 'survived',\n",
       " 'rescue',\n",
       " 'refugees',\n",
       " 'put',\n",
       " 'o',\n",
       " 'girl',\n",
       " 'drought',\n",
       " 'confirmed',\n",
       " 'area',\n",
       " 'air',\n",
       " '30',\n",
       " 'wind',\n",
       " 'injury',\n",
       " 'head',\n",
       " 'bag',\n",
       " 'weapons',\n",
       " 'survivors',\n",
       " 'screaming',\n",
       " 'report',\n",
       " 'released',\n",
       " 'panic',\n",
       " 'kills',\n",
       " 'hurricane',\n",
       " 'hostage',\n",
       " 'hail',\n",
       " 'explosion',\n",
       " 'destroy',\n",
       " 'blood',\n",
       " 'big',\n",
       " 'whole',\n",
       " 'warning',\n",
       " 'save',\n",
       " 'mosque',\n",
       " 'missing',\n",
       " 'burned',\n",
       " 'breaking',\n",
       " 'attacked',\n",
       " 'thing',\n",
       " 'rescued',\n",
       " 'past',\n",
       " 'migrants',\n",
       " 'made',\n",
       " 'heard',\n",
       " 'hazard',\n",
       " 'destruction',\n",
       " 'bridge',\n",
       " 'apocalypse',\n",
       " 'against',\n",
       " '40',\n",
       " 'violent',\n",
       " 'trauma',\n",
       " 'sinking',\n",
       " 'responders',\n",
       " 'rescuers',\n",
       " 'ok',\n",
       " 'island',\n",
       " 'high',\n",
       " 'drowning',\n",
       " 'sinkhole',\n",
       " 'self',\n",
       " 'saw',\n",
       " 'real',\n",
       " 'keep',\n",
       " 'hundreds',\n",
       " 'end',\n",
       " 'dust',\n",
       " 'demolished',\n",
       " 'deaths',\n",
       " 'crush',\n",
       " 'crashed',\n",
       " 'collapsed',\n",
       " 'catastrophic',\n",
       " 'bags',\n",
       " '0',\n",
       " 'white',\n",
       " 'use',\n",
       " 'stock',\n",
       " 'river',\n",
       " 'lives',\n",
       " 'lava',\n",
       " 'hazardous',\n",
       " 'explode',\n",
       " 'evacuate',\n",
       " 'due',\n",
       " 'displaced',\n",
       " 'county',\n",
       " 'collision',\n",
       " 'blown',\n",
       " 'ambulance',\n",
       " 'wreckage',\n",
       " 'wave',\n",
       " 'very',\n",
       " 'update',\n",
       " 'trouble',\n",
       " 'traumatised',\n",
       " 'tragedy',\n",
       " 'structural',\n",
       " 'screamed',\n",
       " 'outbreak',\n",
       " 'must',\n",
       " 'market',\n",
       " 'macre',\n",
       " 'light',\n",
       " 'exploded',\n",
       " 'drown',\n",
       " 'derailment',\n",
       " 'collided',\n",
       " 'china',\n",
       " 'charged',\n",
       " 'catastrophe',\n",
       " 'bombed',\n",
       " 'bang',\n",
       " 'away',\n",
       " 'august',\n",
       " 'wounds',\n",
       " 'screams',\n",
       " 'ruin',\n",
       " 'quarantined',\n",
       " 'quarantine',\n",
       " 'meltdown',\n",
       " 'long',\n",
       " 'least',\n",
       " 'electrocuted',\n",
       " 'calgary',\n",
       " 'blew',\n",
       " 'bleeding',\n",
       " 'battle',\n",
       " 'b',\n",
       " '05',\n",
       " '00',\n",
       " 'three',\n",
       " 'suspect',\n",
       " 'riot',\n",
       " 'part',\n",
       " 'panicking',\n",
       " 'obliteration',\n",
       " 'mudslide',\n",
       " 'investigators',\n",
       " 'inundated',\n",
       " 'harm',\n",
       " 'group',\n",
       " 'flattened',\n",
       " 'engulfed',\n",
       " 'deluged',\n",
       " 'curfew',\n",
       " 'crushed',\n",
       " 'caused',\n",
       " 'blast',\n",
       " 'better',\n",
       " 'anniversary',\n",
       " 'airplane',\n",
       " 'wrecked',\n",
       " 'windstorm',\n",
       " 'twister',\n",
       " 'thought',\n",
       " 'something',\n",
       " 'seismic',\n",
       " 'sandstorm',\n",
       " 'road',\n",
       " 'rioting',\n",
       " 'obliterated',\n",
       " 'obliterate',\n",
       " 'n',\n",
       " 'hostages',\n",
       " 'horrible',\n",
       " 'hijacking',\n",
       " 'half',\n",
       " 'devastation',\n",
       " 'detonation',\n",
       " 'desolation']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_map = make_hash_map(df_raw) #create hash map (words and frequency) from tokenized dataset\n",
    "\n",
    "vocab=frequent_vocab(hash_map, 500)\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment! Change max_feature to 100. Check out what do you get. \n",
    "Remember to change it back to 500 or more afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we build our bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function bagofwords with the following input: sentence and words\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence) #tokenize sentences/ tweets and assign it to variable sentence_words\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words)) #create a NumPy array made up of zeroes with size len(words)\n",
    "    # loop through data and add value of 1 when token is present in the tweet\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag) # return the bag of word for one tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Test your function using a made up text data.\n",
    "Look at the list of words above to see what words you might want to add into your sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 't co http  in for'\n",
    "bagofwords(text, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice your one row of bag of words! \n",
    "\n",
    "Now, we want to loop this function through our whole dataset. See below for how we do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-eaaaee919a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#call out the previous function 'bagofwords'. see the inputs: sentence and words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbag_o\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagofwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# set up a NumPy array with the specified dimension to contain the bag of words\n",
    "n_words = len(vocab)\n",
    "n_docs = len(df_raw)\n",
    "bag_o = np.zeros([n_docs,n_words])\n",
    "# use loop function to add new row for each tweet. \n",
    "for ii in range(n_docs): \n",
    "    #call out the previous function 'bagofwords'. see the inputs: sentence and words\n",
    "    bag_o[ii,:] = bagofwords(df['text'].iloc[ii], vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, find out the [dimension](https://stackoverflow.com/questions/14847457/how-do-i-find-the-length-or-dimensions-size-of-a-numpy-matrix-in-python) of the numpy array. Does it make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bagofwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f6c47c245ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbagofwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bagofwords' is not defined"
     ]
    }
   ],
   "source": [
    "bagofwords.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find out the total frequency, inverse document frequency\n",
    "\n",
    "Here, we would like to work with words that provide us with the most meaning in the sentences/ tweets. Does it make sense to think that the words that are used most often are important?\n",
    "\n",
    "We first take a look at the words inside our bag of words. Print the 20 most frequent words.  \n",
    "hint: Your hash_map is a dictionary of all words with each word as a key, and its frequency as a value aka dict{word: frequency}. What do you notice about these most common words?\n",
    "\n",
    "See [this article](https://docs.python.org/3/howto/sorting.html) under 'Key Functions' to find out. Use object’s indices as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is key=lambda for? See [this article](https://stackoverflow.com/questions/13669252/what-is-key-lambda/13669294) to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the top 20 words above. What do you notice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing features\n",
    "The 20 most common words give almost NO information about the tweets. They are remains of twitter urls, as well as some common words frequently found in all text. We can hardly consider them 'important features'. It seems then that to improve the model, we should do more than just look at the most frequent words. \n",
    "\n",
    "Perhaps we should look for words that appear frequently in some documents, but not in all documents. Why do you think this makes sense?\n",
    "\n",
    "This is the intuition behind what is known as 'total frequency-inverse document frequency', or tfidf.  \n",
    "\n",
    "\n",
    "The tfidf formula is below: \n",
    "$$w_{i,j}=tf_{i,j}*log(\\frac{N}{df_i})$$  \n",
    "In this formula, $i$ is a word indexer and $j$ is a document indexer.  \n",
    "In your bag of words, each row is a document, while each column is the frequency of a word in that document. This is already the 'term frequency' portion of tfidf ($tf_{i,j}$). \n",
    "\n",
    "### Inverse document frequency\n",
    "\n",
    "We now want to calculate the inverse document frequency, which can be understood in the following way: for each word, count the number of documents it appears in, and then take the log of the inverse of that number.  \n",
    "\n",
    "Build the idf vector in 2 parts. \n",
    "1. First, build the word frequency for each word. \n",
    "2. Then divide the number of documents (N) by the word frequency and take the log of the result.\n",
    "\n",
    "Remember what we have done during the acquire phase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize 2 variables representing the number of tweets (numdocs) and the number of tokens/words (numwords)\n",
    "numdocs, numwords = np.shape(bag_o)\n",
    "\n",
    "#Changing into the tfidf formula as above\n",
    "N = numdocs\n",
    "word_frequency = np.empty(numwords)\n",
    "\n",
    "#Count the number of documents the word appears in.\n",
    "for word in range(#your code here):\n",
    "    word_frequency[word]=np.sum((bag_o[:,word]>0)) \n",
    "\n",
    "idf = np.log(N/word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will complete our tfidf by nultiplying our bag of words (term frequency) with the idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializs tfidf array\n",
    "tfidf = np.empty([#your code here])\n",
    "\n",
    "#loop through the tweets, multiply term frequency (represented by bag of words) with idf\n",
    "for doc in range(#your code here):\n",
    "    tfidf[doc, :]=bag_o[doc, :]*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you describe the tfidf array? It is made of the tfidf value of each of the 500 token in the 10860 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train your model with Machine Learning\n",
    "Now that you finally have your tfidf array. It is time to train your model and to make predictions! We will be using the scikit learn library, which provides a number of machine learning models. \n",
    "\n",
    "Do you remember what is machine learning? It is an application of AI that allows a system to automatically learn without being explicitly programmed. \n",
    "\n",
    "Now, we are using what we call the supervised learning. Do you remember what this is?\n",
    "This is the type of learning that enable us to make models to predict certain system, given a given training set. In this case, we want to predict if a text relates to news about disaster or not. We have already downloaded text data from twitter, and we have labelled the data with several labels i.e. 'Relevant', 'Not Relevant', and 'Can't decide'. These data will be used to train our model. Let's find out how we can do this!\n",
    "\n",
    "Let's first download the libraries required to do this. The scikit learn library contains numerous useful functions to be used for machine learning problem. \n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #to import logistic regression model\n",
    "from sklearn.model_selection import train_test_split #to split data into training and testing set\n",
    "from sklearn.model_selection import GridSearchCV #to find out the best parameter for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Split data into training and test set\n",
    "\n",
    "Before training our model, we will split our dataset into 2: a training set, and a test set.\n",
    "\n",
    "We will then train our model on the training set, and then test the model generated during the training stage on the test set. This is to ensure that the testing is done on dataset that the model has never 'seen'/ processed. \n",
    "\n",
    "A good starting point for the split is to have 80% of your data in the train set, and 20% of the data in the test set.  \n",
    "\n",
    "Let's do this now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X_all and y_all into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(tfidf,df['relevance'].values,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you explain what is happening on the code above?. We are using the train_test_split function to split the tfidf array and part of the initial dataframe which include the 'relevance' value of our tweet. What does [shuffle=True](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) means?\n",
    "\n",
    "Let's learn more about the dataset we are working with! print tfidf and df['relevance'] below to see them. Meanwhile, find out what is [.value](https://www.geeksforgeeks.org/python-pandas-dataframe-values/) for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have already split our dataset, let's see what data we have now. Print the shape of X_train, X_test, y_train and y_test! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare X_train and X_test, and also compare y_train and y_test. What do you notice about the difference in shape? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Create a model instance\n",
    "After splitting the data, we will make an instance of the model i.e. we are simply initialising the model. In this task, we are using the logistic regressor, which is useful when we want to categorise data. Read [here](https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102) to find out more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model instance\n",
    "logreg = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Train the model on data, store the information learnt from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fit the model on the training set\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not worry about understanding the parameters shown above for now. You can still create projects with the logistic regressor even if you do not know every parameter involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Use model to predict relevance based on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=logreg.predict(X_test)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the y_pred mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now measure our model performance by finding out the accuracy value of the model. Remember, accuracy is defined as:\n",
    "\n",
    "Fraction of correct predictions = correct predictions / total number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logreg.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! we have collected our data, process them, split them into training and testing data, train our model, and evaluate performance of our model. Next, we will define a function that will help us do all these task in one go! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Pipeline\n",
    "\n",
    "Your task is to build a function that will:\n",
    "1. Take in the untrained model, tfidf array, and the values from the training target.\n",
    "2. Randomly split the two into a train and test set\n",
    "3. Fit the model on the training set\n",
    "4. Print the accuracy score on the test set\n",
    "5. Return the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(rf, X_all, y_all): #Take in the untrained model, tfidf array, and the values from the training target.\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_all,y_all,shuffle=True) #Randomly split the two into a train and test set\n",
    "    logreg.fit(X_train,y_train) #Fit the model on the training set\n",
    "    print(rf.score(X_test,y_test)) #Print the score on the test set\n",
    "    return logreg #Return the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the function into our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver = 'lbfgs')\n",
    "#logreg = LogisticRegression()\n",
    "\n",
    "X_all = tfidf\n",
    "y_all = df['relevance'].values\n",
    "logreg = classify(logreg, X_all, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters (Optional)\n",
    "If you have implemented this right, you should have a test score of at least 0.75. \n",
    "\n",
    "Let us try to improve this score by tuning the hyperparameters. We first take a look at the parameters in your logistic regressor. Feel free to read more about its parameters [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Again, don't stress yourself out if you do not understand these parameters now. It will come with more reading and practise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be automatically tuned using scikit-learn's GridSearchCV. This function takes in a model, as well as a dictionary of parameters with values to test, and runs tests each combination of parameters to find the optimal combination for the highest score. Learn more [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your hyperparameters here\n",
    "parameters = {'C':[0.001, 0.01, 0.1, 1, 10], 'tol':[0.0001, 0.001, 0.01], 'max_iter':[100, 1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(logreg, parameters, cv=3, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can view the raw results using clf.cv_results_\n",
    "print(clf.best_params_, clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the accuracy result. Do you notice that it drops? Why do think this is so?\n",
    "\n",
    "The accuracy might be different because we split our models randomly. What we can do is to repeat the model fitting multiple times and get the average accuracy result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build your pipeline\n",
    "At this point you have a trained and optimized model ready to predict tweets. You will now tie it all together to build a pipeline for prediction. \n",
    "\n",
    "This will be a function that:\n",
    "1. Take in a tweet in the form of a string\n",
    "2. Prints a prediction on whether the tweet is relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_predictor(tweet):\n",
    "    word_vector = # your code here # set a variable with bag of words. Remember the bagofwords function you have created?\n",
    "    word_tfidf = # your code here #find tfidf value\n",
    "    prediction = logreg.predict(word_tfidf.reshape(1, -1)) #predict wether a tweet is relevant or not relevant to natural disaster\n",
    "    results = {1:'Relevant', 0:'Not Relevant'} #creating a set containing the potential results. You can change the 'Relevant' and 'Not relevant' tag\n",
    "    print(results[int(prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1 = 'When the aftershock happened (Nepal) we were the last intl team still there; in a way we were 1st responders'\n",
    "tweet2 = 'NLP is fun and I learnt so much today.'\n",
    "twitter_predictor(tweet1)\n",
    "twitter_predictor(tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Write your own tweets and see if your model can classify them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your model able to give you the right result? \n",
    "\n",
    "What do you think we can do to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now built your very own machine learning NLP model.  \n",
    "  \n",
    "# 5. NLP classification challenge!\n",
    "Now that you have learnt the basics of using the bag-of-words normalized by TFIDF to do classification of natural language data, it is time put your skills to the test!  \n",
    "\n",
    "## Sentiment analysis\n",
    "An important application of the classification of natural text is in _sentiment analysis_. Sentiment analysis is the process of categorizing opinions in a piece of text in order to identify the writer's attitude toward a particular topic.  \n",
    "  \n",
    "In this challenge, we will be classifying movie reviews from [imdb](https://www.imdb.com/). The data has already been stored as two .pkl files (for now, just understand these as file types that can be read using python), one for the training data, and one on the test data. \n",
    "\n",
    "You will have to process and train your model on the train dataset of movie reviews `df_raw.pkl`, and then report the accuracy of your model on the test move reviews `df_raw_test.pkl`.  \n",
    "  \n",
    "You will be predicting if a review has either a positive or negative sentiment. Positive sentiments are labeled as 1, while negative sentiments are labeled as 0.\n",
    "  \n",
    "In this segment we utilize a few new functions provided by sklearn. \n",
    "1. You can generate your bag of words and condition it using TFIDF with the functions you have built earlier. \n",
    "2. Alternatively, the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) function can be used to create your bag of words. Use the `max_features=5000` argument to only select the top 5000 most common words. \n",
    "3. You can also use the [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) to help transform your bag of words with TFIDF.  \n",
    "  \n",
    "The dataframes for the train and test dataset has already been imported for you. You will have to make use of the skills you have learnt earlier and in Experience 1 to preprocess, vectorize (with the bag of words), transform (with TFIDF), fit, and predict the movie review sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer # This function helps you create your bag of words\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # This function automatically normalizes your bag of words\n",
    "df_raw = pd.read_pickle('./imdb/df_raw.pkl')\n",
    "df_raw_test = pd.read_pickle('./imdb/df_raw_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, try printing a sample of the test dataset. What are the names of the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, process your text using the `CountVectorizer` to create your bag of words. you can create a class of `CountVectorizer()`, and use the method `.fit_transform()` with the text as argument to build your bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", strip_accents=None, tokenizer = None, \\\n",
    "                             preprocessor = None, stop_words = None, max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(df_raw['text'])\n",
    "test_data_features = vectorizer.transform(df_raw_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now normalize your bag of words using the `TfidfTransformer`. Use it the same way as above. Create a class, and use the `.fit_transform()` method with the bag of words as your argument to create your TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfier = TfidfTransformer()\n",
    "tfidf = tfidfier.fit_transform(train_data_features)\n",
    "tfidf_test = tfidfier.transform(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use your transformed bag of words as the features to train and test your model like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = tfidf.toarray()\n",
    "y_all = df_raw['positive'].values\n",
    "X_test = tfidf_test.toarray()\n",
    "y_test = df_raw_test['positive'].values\n",
    "def classify():\n",
    "    rf = LogisticRegression()\n",
    "    rf.fit(X_all,y_all)\n",
    "    print(rf.score(X_test,y_test))\n",
    "    return rf\n",
    "classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have an accuracy of 80% without any hyperparameter tuning. Try and get the best accuracy on the test set that you can! This dataset is one of the hallmarks of natural language processing and is the entry-point for many aspiring data scientists and engineers. You can [see the original competition here](https://www.kaggle.com/c/word2vec-nlp-tutorial), and look at the different solutions other people have created!  \n",
    "\n",
    "## Next up, you can create a function to input your own review and get the model to predict if your sentence is positive or negative!\n",
    "\n",
    "# Congratulations!\n",
    "You have learnt how to build a natural language text classifier! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
